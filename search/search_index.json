{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang di situs Penambangan Data. \u00b6 Situs yang menampilkan rangkuman seputar Data Mining beserta penerapannya. Profil : Nama : Rizky Alfriansyah NRP : 180411100068 Kelas : Penambangan Data 5E Studi : Teknik Informatika Universitas Trunojoyo Madura Domisili: Sidoarjo","title":"Memo"},{"location":"#selamat-datang-di-situs-penambangan-data","text":"Situs yang menampilkan rangkuman seputar Data Mining beserta penerapannya. Profil : Nama : Rizky Alfriansyah NRP : 180411100068 Kelas : Penambangan Data 5E Studi : Teknik Informatika Universitas Trunojoyo Madura Domisili: Sidoarjo","title":"Selamat Datang di situs Penambangan Data."},{"location":"More ActionsNo/","text":"No. GameID LeagueIndex Age HoursPerWeek TotalHours APM SelectByHotkeys AssignToHotkeys UniqueHotkeys MinimapAttacks MinimapRightClicks NumberOfPACs GapBetweenPACs ActionLatency ActionsInPAC TotalMapExplored WorkersMade UniqueUnitsMade ComplexUnitsMade ComplexAbilitiesUsed 3365 10050 8 ? ? ? 211.9188 0.019817 0.000633 4 0.000201 0.000201 0.003912 31.8222 54.5588 5.0294 14 0.001409 3 0.000000 0.000000 3366 10051 8 ? ? ? 269.8998 0.024645 0.000642 10 0.000415 0.000491 0.004015 25.6352 43.3856 6.4922 21 0.000478 6 0.000000 0.000579 3367 10052 8 ? ? ? 190.2396 0.008720 0.000879 10 0.000171 0.000342 0.004971 17.9901 35.9509 5.5872 21 0.000904 5 0.000000 0.000000 3368 10055 8 ? ? ? 212.4972 0.014917 0.000767 10 0.000599 0.000273 0.005648 21.6687 41.2231 4.4680 28 0.001119 9 0.000035 0.000062 3369 10059 8 ? ? ? 219.3894 0.005926 0.000741 6 0.000440 0.000709 0.005185 17.0456 30.5342 6.6749 35 0.002072 9 0.000225 0.000064 3370 10060 8 ? ? ? 230.6694 0.010383 0.001242 10 0.000375 0.003328 0.006375 13.5028 31.4044 5.0533 32 0.001512 6 0.000035 0.000047 3371 10061 8 ? ? ? 284.2296 0.016069 0.000711 9 0.000355 0.000548 0.006680 9.4756 29.6851 5.3326 25 0.002459 7 0.000000 0.000000 3372 10062 8 ? ? ? 355.3518 0.037526 0.000600 7 0.001242 0.000514 0.004541 9.2871 41.9497 6.5063 22 0.001228 8 0.000000 0.000614 3373 10063 8 ? ? ? 364.8504 0.042576 0.000996 8 0.000176 0.000146 0.004687 19.9499 41.1417 5.6167 18 0.000674 7 0.000000 0.000000 3374 10064 8 ? ? ? 256.5888 0.019592 0.000580 8 0.000416 0.000357 0.005812 17.0462 34.3734 5.0563 19 0.001308 7 0.000000 0.000000 3375 10065 8 ? ? ? 248.4012 0.016018 0.000874 9 0.000388 0.000372 0.005987 16.3144 30.2486 5.0973 21 0.001197 6 0.000000 0.000000 3376 10066 8 ? ? ? 251.2284 0.022910 0.000946 5 0.001097 0.001173 0.005411 13.7404 35.7203 4.5524 22 0.000738 5 0.000000 0.000662","title":"More ActionsNo"},{"location":"Out6/","text":"Out[6]: Statistics Jatim Park 1 Jatim Park 2 Jatim Park 3 Kenjeran Park Suroboyo Carnival 0 Count 100.00 100.00 100.00 100.00 100.00 1 Max 448.00 450.00 450.00 449.00 432.00 2 Min 186.00 188.00 190.00 190.00 190.00 3 Mean 314.88 308.42 329.94 330.27 304.81 4 Standard Deviasi 73.77 69.25 77.44 73.60 69.53 5 Variasi 5442.19 4795.64 5996.80 5416.97 4833.75 6 Quantile 1 253.00 250.00 264.00 271.25 242.50 7 Quantile 2 324.00 309.00 338.00 340.50 304.50 8 Quantile 3 371.50 363.00 393.50 387.25 351.75 9 Skewnes -0.10 0.12 -0.14 -0.26 0.14 10 Median 324.00 309.00 338.00 340.50 304.50 11 Modus 199.00 257.00 346.00 326.00 273.00","title":"Out6"},{"location":"Regresi Linier Sederhana dan Berganda/","text":"Regresi Linier Sederhana dan Berganda \u00b6 Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y). Regresi Linier Sederhana \u00b6 Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut : Regresi Linier Berganda \u00b6 Regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan) Contoh kasus dengan penghitungan manual \u00b6 Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi. Langkah 1 : Penentuan Tujuan \u00b6 Tujuan : Memprediksi Jumlah cacat produksi jika suhu ruangan tidak terkendali Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat \u00b6 **Varibel Faktor Penyebab (X) ** : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi Langkah 3 : Pengumpulan Data \u00b6 Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14 Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya \u00b6 Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya : Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana \u00b6 Langkah 6 : Buat Model Persamaan Regresi \u00b6 Y = a + bX Y = -24,38 + 1,45X Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat \u00b6 I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C Contoh kasus dengan penghitungan sklearn \u00b6 Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Regresi Linier Sederhana dan Berganda"},{"location":"Regresi Linier Sederhana dan Berganda/#regresi-linier-sederhana-dan-berganda","text":"Dalam berbagai penulisan laporan penelitian ilmiah, analisis regresi banyak sekali digunakan. Bahkan, bisa jadi, analisis ini paling banyak digunakan. Analisis regresi menunjukkan pengaruh variabel independen atau variabel bebas (X) terhadap variabel dependen atau variabel tergantung (Y). Dengan demikian, setidaknya ada 2 variabel yang terlibat dalam uji atau analisis regresi yaitu 1). variabel independen atau variabel bebas (X), dan 2) variabel dependen atau variabel tergantung (Y).","title":"Regresi Linier Sederhana dan Berganda"},{"location":"Regresi Linier Sederhana dan Berganda/#regresi-linier-sederhana","text":"Regresi linier sederhana adalah regresi yang hanya melibatkan dua variabel, yaitu 1 (satu) variabel dependen atau variabel tergantung dan 1 (satu) variabel independen atau bebas. Persamaan di atas adalah rumus dari persamaan regresi linear sederhana. Y adalah variabel tak bebas, a adalah koefisien intersep, b adalah kemiringan dan t adalah variabel bebas. Rumus untuk b adalah : Dan rumus untuk mendapatkan nilai a adalah sebagai berikut : Dalam regresi linear sederhana juga ada yang disebut dengan koefisien korelasi yang menunjukkan bahwa nilai suatu variabel bergantung pada perubahan nilai variabel yang lain. Rumus untuk menghitung koefisien korelasi adalah sebagai berikut :","title":"Regresi Linier Sederhana"},{"location":"Regresi Linier Sederhana dan Berganda/#regresi-linier-berganda","text":"Regresi linier berganda adalah hubungan secara linear antara dua atau lebih variabel independen (X1, X2,\u2026.Xn) dengan variabel dependen (Y). Analisis ini untuk mengetahui arah hubungan antara variabel independen dengan variabel dependen apakah masing-masing variabel independen berhubungan positif atau negatif dan untuk memprediksi nilai dari variabel dependen apabila nilai variabel independen mengalami kenaikan atau penurunan. Data yang digunakan biasanya berskala interval atau rasio. Persamaan regresi linear berganda sebagai berikut: Y\u2019 = a + b1X1+ b2X2+\u2026..+ bnXn Keterangan: Y\u2019 = Variabel dependen (nilai yang diprediksikan) X1 dan X2 = Variabel independen a = Konstanta (nilai Y\u2019 apabila X1, X2\u2026..Xn = 0) b = Koefisien regresi (nilai peningkatan ataupun penurunan)","title":"Regresi Linier Berganda"},{"location":"Regresi Linier Sederhana dan Berganda/#contoh-kasus-dengan-penghitungan-manual","text":"Seorang Engineer ingin mempelajari Hubungan antara Suhu Ruangan dengan Jumlah Cacat yang diakibatkannya, sehingga dapat memprediksi atau meramalkan jumlah cacat produksi jika suhu ruangan tersebut tidak terkendali. Engineer tersebut kemudian mengambil data selama 30 hari terhadap rata-rata (mean) suhu ruangan dan Jumlah Cacat Produksi.","title":"Contoh kasus dengan penghitungan manual"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-1-penentuan-tujuan","text":"Tujuan : Memprediksi Jumlah cacat produksi jika suhu ruangan tidak terkendali","title":"Langkah 1 : Penentuan Tujuan"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-2-identifikasikan-variabel-penyebab-dan-akibat","text":"**Varibel Faktor Penyebab (X) ** : Suhu Ruangan, Variabel Akibat (Y) : Jumlah Cacat Produksi","title":"Langkah 2 : Identifikasikan Variabel Penyebab dan Akibat"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-3-pengumpulan-data","text":"Berikut ini adalah data yang berhasil dikumpulkan selama 30 hari (berbentuk tabel) : Tanggal Rata-rata Suhu Ruangan Jumlah Cacat 1 24 10 2 22 5 3 21 6 4 20 3 5 22 6 6 19 4 7 20 5 8 23 9 9 24 11 10 25 13 11 21 7 12 20 4 13 20 6 14 19 3 15 25 12 16 27 13 17 28 16 18 25 12 19 26 14 20 24 12 21 27 16 22 23 9 23 24 13 24 23 11 25 22 7 26 21 5 27 26 12 28 25 11 29 26 13 30 27 14","title":"Langkah 3 : Pengumpulan Data"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-4-hitung-x2-y2-xy-dan-total-dari-masing-masingnya","text":"Berikut ini adalah tabel yang telah dilakukan perhitungan X\u00b2, Y\u00b2, XY dan totalnya :","title":"Langkah 4 : Hitung X\u00b2, Y\u00b2, XY dan total dari masing-masingnya"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-5-hitung-a-dan-b-berdasarkan-rumus-regresi-linear-sederhana","text":"","title":"Langkah 5 : Hitung a dan b berdasarkan rumus Regresi Linear Sederhana"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-6-buat-model-persamaan-regresi","text":"Y = a + bX Y = -24,38 + 1,45X","title":"Langkah 6 : Buat Model Persamaan Regresi"},{"location":"Regresi Linier Sederhana dan Berganda/#langkah-7-lakukan-prediksi-atau-peramalan-terhadap-variabel-faktor-penyebab-atau-variabel-akibat","text":"I. Prediksikan Jumlah Cacat Produksi jika suhu dalam keadaan tinggi (Variabel X), contohnya : 30\u00b0C Y = -24,38 + 1,45 (30) Y = 19,12 Jadi Jika Suhu ruangan mencapai 30\u00b0C, maka akan diprediksikan akan terdapat 19,12 unit cacat yang dihasilkan oleh produksi. II. Jika Cacat Produksi (Variabel Y) yang ditargetkan hanya boleh 4 unit, maka berapakah suhu ruangan yang diperlukan untuk mencapai target tersebut ? 4 = -24,38 + 1,45X 1,45X = 4 + 24,38 X = 28,38 / 1,45X = 19,57 Jadi Prediksi Suhu Ruangan yang paling sesuai untuk mencapai target Cacat Produksi adalah sekitar 19,57\u00b0C","title":"Langkah 7 : Lakukan Prediksi atau Peramalan terhadap Variabel Faktor Penyebab atau Variabel Akibat"},{"location":"Regresi Linier Sederhana dan Berganda/#contoh-kasus-dengan-penghitungan-sklearn","text":"Dalam pembelajaran kali ini, kita ingin mencari solusi dari proses perekrutan sebuah perusahaan. Perusahaan ini sedang merekrut seorang calon pegawai baru. Namun, bagian HRD perusahaan ini kebingungan, berapa gaji yang harus ia berikan, sesuai dengan level di mana calon pegawai baru ini masuk. Tentunya akan ada proses negosiasi antara HRD dengan calon pegawai baru ini tentang jumlah gaji yang pantas diterima pegawai tersebut. Calon pegawai ini mengaku bahwa sebelumnya ia telah berada di posisi Region Manager dengan pengalaman bekerja 20 tahun lebih dengan gaji hampir 160K dollar per tahun. Ia meminta perusahaan baru ini untuk memberikan ia gaji lebih dari 160K dollar per tahun. Untuk menyelidiki apakah calon pegawai ini benar-benar digaji sebanyak 160K dollar/tahun, maka bagian HRD membandingkan data gaji perusahaan tempat calon pegawai ini bekerja sebelumnya (kebetulan perusahaan memiliki daftar gajinya) dengan pengakuannya. Data yang dimiliki adalah daftar antara gaji dan level di perusahaan tersebut. Bagian HRD ingin mencari hubungan antara gaji yang didapat dengan level (tingkatan jabatan) di perusahaan calon pekerja tadi bekerja sebelumnya. Hasil penelitian awal, calon pegawai ini layak masuk di level 6.5 (antara region manager dan partner ). Berikut variabel yang kita miliki: Variabel dependen : Gaji (dalam dollar per tahun) Variabel independen : level (tingkatan jabatan) Setelah melihat tabelnya, bisa dilihat bahwa kita memiliki 1 variabel dependen, dan 1 variabel independen. Dari sini kita bisa tahu bahwa kita bisa menggunakan pendekatan model regresi sederhana. Walau demikian, datanya sudah diatur sedemikian rupa sehingga fungsi yang dimiliki antara variabel dependen dengan independen adalah kuadratik. Kita tetap akan mencoba membuat 2 model (simple dan polinomial) untuk membandingkan performanya (seberapa fit antara 2 model regresi ini dengan data). # Mengimpor library import`` numpy as np import`` matplotlib.pyplot as plt import`` pandas as pd Line 2 sampai line 4 mengimpor library yang diperlukan # Mengimpor dataset dataset ``=`` pd.read_csv(``'Posisi_gaji.csv'``) X ``=`` dataset.iloc[:, ``1``:``2``].values y ``=`` dataset.iloc[:, ``2``].values Line 7 mengimpor datasetnya Line 8 menentukan variabel independen X. Penting, bahwa usahakan variabel independen adalah matrix, dan bukan vector. Kita bisa saja menuliskan X = dataset.iloc[:, 1].values , namun perintah ini akan menghasilkan vector. Biasakan membuatnya sebagai sebuah matrix, dengan cara melakukan slicing X = dataset.iloc[:, 1:2].values . Bagaimana kita tahu X sudah menjadi matrix? Bisa dilihat kolom size di spyder variabel X adalah (10,1). Artinya X adalah matrix 10\u00d71 (10 baris dan 1 kolom). Line 9 menentukan variabel dependen y. Penting, usahakan variabel dependen adalah vector. Vektor ( vector ) adalah matriks yang hanya terdiri dari 1 kolom, atau matriks 1 baris. Cara membuatnya menjadi vektor adalah jangan lakukan slicing pada bagian kolomnya. Pada bagian size variabel y di spyder adalah (10,) yang artinya ia adalah matrix 1 baris. # Fitting Linear Regression ke dataset from`` sklearn.linear_model ``import`` LinearRegression lin_reg ``=`` LinearRegression() lin_reg.fit(X, y) Line 12 mengimpor class LinearRegression (untuk membuat model regresi sederhana) Line 13 mempersiapkan objek lin_reg sebagai model regresi sederhana Line 14 membuat model regresi sederhana (Kali ini tanpa membagi dataset ke dalam test dan train set, karena datasetnya terlalu kecil (biasanya train set minimal butuh 10 baris, dan kali ini tidak cukup data untuk dimasukkan ke test set). Walau demikian, model yang jadi nanti akan merupakan bagian dari train set, dan dataset baru yang diterima (pengujian train set) akan menjadi test set-nya). # Fitting Polynomial Regression ke dataset from`` sklearn.preprocessing ``import`` PolynomialFeatures poly_reg ``=`` PolynomialFeatures(degree ``=`` ``2``) ``## nantinya degree diganti menjadi 4 X_poly ``=`` poly_reg.fit_transform(X) lin_reg_2 ``=`` LinearRegression() lin_reg_2.fit(X_poly, y) Line 17 mengimpor PolynomialFeatures dari library sklearn.preprocessing untuk membuat model polinomial. Untuk mengetahui parameter apa saja yang diperlukan, cukup arahkan kursor pada PolynomialFeatures, lalu klik CTRL+i. Line 18 mempersiapkan objek poly_reg sebagai transformasi matriks X menjadi matriks X pangkat 2, pangkat 3 hingga pangkat n. Jadi nantinya kita memiliki beberapa tambahan variabel independen sebanyak n. Parameter default untuk PolynomialFeatures adalah degrees=2. Line 19 menyiapkan objek X_poly sebagai hasil fit_transform (proses fit dan transform dilakukan sekaligus) dari variabel X. Mari kita bandingkan antara X dengan X_poly. Line 20 menyiapkan objek lin_reg_2 sebagai model regresi polinomial. Line 21 membuat model regresi polinomial dengan parameter variabel independen adalah X_poly, dan variabel dependennya adalah y. # Visualisasi hasil regresi sederhana plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg.predict(X), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Linear Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 24 sampai line 29 adalah perintah untuk visualisasi hasil model regresi sederhana kita. Ingat untuk visualisasi, perintah dari line 24-29 harus dieksekusi bersamaan. Visualisasinya akan nampak sebagai berikut : # Visualisasi hasil regresi polynomial plt.scatter(X, y, color ``=`` ``'red'``) plt.plot(X, lin_reg_2.predict(X_poly), color ``=`` ``'blue'``) plt.title(``'Sesuai atau tidak (Polynomial Regression)'``) plt.xlabel(``'Level posisi'``) plt.ylabel(``'Gaji'``) plt.show() Line 32 sampai line 37 adalah perintah untuk visualisasi hasil model regresi polinomial. Pelru diingat sumbu y nya adalah lin_reg_2.predict(X_poly) . Hasilnya akan tampak sebagai berikut : Bisa dilihat dengan menggunakan fungsi polinomial hasilnya cukup baik. Namun tetap saja masih kurang cukup fit , di mana masih ada jarak antara model dengan data. Solusinya adalah pada line 18 kita ubah degree nya dari 2 menjadi 4. Eksekusi line 18 sampai line 21. Kemudian eksekusi line 32 sampai line 37. Maka visualisasi yang baru akan tampak sebagai berikut : # Memprediksi hasil dengan regresi sederhana lin_reg.predict(``6.5``) Line 40 adalah perintah untuk melihat dengan model regresi sederhana yang sudah dibuat, berapa gaji yang layak untuk tingkat level 6.5? Maka cukup ganti parameter X di lin_reg.predict(X) dengan angka 6.5. Jika dieksekusi, hasilnya adalah 330378.78 dollar/tahun. Tentunya prediksi dari regresi sederhana terlalu tinggi (terlihat juga di plot visualisasinya). Kita tidak menginginkan gaji yang terlalu tinggi yang merupakan hasil dari model regresi sederhana yang buruk kali ini. # Memprediksi hasil dengan regresi polynomial lin_reg_2.predict(poly_reg.fit_transform(``6.5``)) Line 43 adalah perintah untuk melihat prediksi gaji dengan model regresi polinomial. Perlu diperhatikan bahwa parameter X diganti dengan poly_reg.fit_transform(6.5) dan bukan X_poly. Karena kita ingin mengisi angka 6.5 sebagai parameter X. Sementara X_poky adalah hasil dari definisi fungsi poly_reg.fit_transform(X). Ketika dieksekusi maka hasilnya adalah 158862.45 dollar/tahun. Prediksi yang cukup baik, dengan model yang fit. MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Contoh kasus dengan penghitungan sklearn"},{"location":"Statistik/","text":"Statistika Deskriptif \u00b6 Pengertian Statistika \u00b6 Statistika yaitu merupakan suatu ilmu yang mempelajari tentang cara-cara pengumpulan data, penyajian data, dan juga analisis serta interpretasi tentang data tersebut sehingga menjadi suatu informasi yang berguna bagi pengambilan keputusan. Diperlukan sebuah metode. Metode Statistika adalah prosedur-prosedur yang digunakan dalam pengumpulan, penyajian, analisis, dan penafsiran data. \u200b Pengertian Statistika Deskriptif \u00b6 Statistik yang menggambarkan kegiatan berupa pengumpulan data, penyusunan data, pengelolaan data, dan penyajian data dalam bentuk tabel, grafik, ataupun diagram, agar memberikan gambaran yang teratur ringkas, dan jelas mengenai suatu keadaan atau peristiwa. (M. Subana dkk, 2000;12) Deskriptif sendiri memiliki sifat menggambarkan atau mendeskripsikan suatu kondisi. Statistika deskriptif memiliki fungsi mempelajari tata cara pengumpulan, pencatatan, penyusunan, dan penyajian data penelitian dalam bentuk tabel frekuensi atau grafik, dan selanjutnya dilakukan pengukuran nilai-nilai statistiknya seperti mean/ rata-rata. \u200b Distribusi Data \u00b6 Pengaturan, penyusunan, dan peringkasan data dengan membuat tabel seringkali membantu, terutama pada saat kita bekerja dengan sejumlah data yang besar. Tabel tersebut berisi daftar nilai yang mungkin berbeda (baik data tunggal maupun data berkelompok ) beserta nilai frekuensinya. Frekuensi adalah banyaknya kejadian/munculnya nilai data dengan kategori tertentu. Data yang telah diatur disebut **distribusi frekuensi**. Distribusi frekuensi dapat digambarkan dengan dua cara, yaitu sebagai **tabel** ataupun sebuah **grafik**. Grafik data kuantitatif dapat meliputi Histogram , Poligon frekuensi , dll. Sedangkan grafik untuk data kualitatif meliputi Bar Chart , Pie Chart , dll. Ukuran Pemusatan \u00b6 Mean \u00b6 Merupakan metode yang paling banyak digunakan untuk menggambarkan ukuran tendensi sentral. Mean dihitung dengan menjumlahkan semua nilai data lalu dibagi dengan banyaknya data tersebut. Berikut untuk rumus Mean: $$ \\overline{x}=\\frac{x_{1} f_{1}+x_{2} f_{2}+\\ldots+x_{n} f_{n}}{f_{1}+f_{2}+\\ldots+f_{n}} \\text { atau } \\quad \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i} f_{i}}{\\sum_{i=1}^{n} f_{i}} $$ keterangan: x bar = hasil mean xi .fi = jumlah data fi = banyak data Mode \u00b6 Mode atau yang lebih sering dikenal dengan modus, adalah data yang sering muncul. Penentuannya dengan menyusun data dari kecil ke besar atau sebaliknya, kemudian hitung berapa banyak data yang sama tersebut. Berikut untuk rumus mode : $$ Modus =t b+\\left(\\frac{\\Delta F 1}{\\Delta F 1+\\Delta F 2}\\right) \\mathrm{p} $$ Keterangan: Modus = Modus tb = tepi bawah delta F1 = Frekuensi tertinggi - frekuensi diatasnya delta F2 = Frekuensi tertinggi - frekuensi diatasnya p = interval Median \u00b6 Median adalah nilai tengah. DImana pengkuran pemusatan ini dengan membagi atau mencari nilai tengahnya, atau data yang berada di tengah. Berikut untuk rumus median: $$ M e=Q_{2}=\\left(\\frac{x_{n+1}}{2}\\right), jikanganjil $$ Keterangan: Q2 = Quartal 2 ( 50%) $$ M e=Q_{2}=\\left(\\frac{\\frac{x_{n} x_{n+1}}{2}}{2}\\right), jikangenap $$ Keterangan: perlu dibagi dua untuk menemukan nilai tengah Skewness \u00b6 Dibutuhkan karena memiliki manfaat tersendiri, yaitu akan mengetahui grafik normalitas melenceng ke kanan atau kekiri, mengumpul terlalu ditengah atau terlalu datar. Biasa juga disebut dengan ukuran kemencengan data. Rumus skewness: $$ \\mathrm{Sk}=\\frac{\\overline{\\mathrm{x}}-\\mathrm{Mo}}{\\mathrm{s}} $$ Keterangan: Sk = Skewness x = rata-rata Mo = Modus s = Simpangan baku Standar Deviasi \u00b6 Adalah nilai yang digunakan untuk menentukan sebaran data sampel, serta untuk mengetahui seberapa dekat titik titik antara mean (rata-rata) nilai sampel. Untuk menghitung standar deviasi, ahli statistik pertama-tama menghitung nilai rata-rata dari semua titik data. Deviasi yang telah dihasilkan dikenal dengan variasi. Deviasi adalah akar kuadrat dari varians. Rumus Standar Deviasi: $$ s=\\sqrt{s^{2}} $$ Keterangan: s = Standar Deviasi s^2 = varians s=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\overline{x}\\right)^{2}} s=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\overline{x}\\right)^{2}} Keterangan: s = Standar Deviasi N = Jumlah data i = nomor data xi = data ke-i x bar = rata-rata sampel Varians \u00b6 Merupakan ukuran penyimpangan(selisih) antara nilai normal yang diharapkan menjadi kenyataan. Rumus Varians: s^{2}=\\frac{n \\sum_{i=1}^{n} x_{i}^{2}-\\left(\\sum_{i=1}^{n} x_{1}\\right)^{2}}{n(n-1)} s^{2}=\\frac{n \\sum_{i=1}^{n} x_{i}^{2}-\\left(\\sum_{i=1}^{n} x_{1}\\right)^{2}}{n(n-1)} Keterangan: s^2 : variasi n = banyaknya data xi^2 = data ke-i kuadrat Quartile \u00b6 Kuartil adalah suatu rumus yang dapat memisahkan tiga bagian. Yang pertama ada kuartil bawah(Q1). Kuartil bawah diambil dari banyaknya data dikali 0.25. Kuartil tengah dikali 0.5 Kuartil atas dikali 0.75. Rumus mencari kuartil: Q_{i}=\\frac{i(n+1)}{4} Q_{i}=\\frac{i(n+1)}{4} Keterangan: untuk menentukan Quartil 1 i = 1 $$ \\quad Q_{i}=\\frac{i(n+1)}{4} $$ Keterangan: untuk menentukan Quartil 2 i = 2 $$ \\quad Q_{i}=\\frac{i(n+1)}{4}$ $$ Keterangan: untuk menentukan Quartil 3 i = 3 Penerapan Statistika Deskriptif dengan Python \u00b6 Alat dan Bahan \u00b6 Text Editor (Jupyter Notebook paling direkomendasikan) Library Pandas (untuk statistika) Library scipy (untuk algoritma serta fungsi matematika) Pertama \u00b6 import pandas as pd from scipy import stats Kedua \u00b6 Import file data yang akan dieksekusi (direkomendasikan menggunakan format .csv) df = pd . read_csv ( 'dataPWJT.csv' ) Ketiga \u00b6 Membuat tampungan data dengan dictionary lalu melakukan iterasi untuk mendapatkan data-data setiap barisnya. data = { \"Statistics\" : [ 'Count' , 'Max' , 'Min' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Skewnes' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . count (), df [ i ] . max (), df [ i ] . min (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), round ( df [ i ] . skew (), 2 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] Keempat \u00b6 Eksekusi dengan dataframe agar data yang dihasilkan dapat dilihat dengan baik. eksekusi = pd . DataFrame ( data , columns = [ 'Statistics' ] + [ x for x in df . columns ]) eksekusi Hasilnya: Statistics Jatim Park 1 Jatim Park 2 Jatim Park 3 Kenjeran Park 0 Count 100.00 100.00 100.00 100.00 1 Max 448.00 450.00 450.00 449.00 2 Min 186.00 188.00 190.00 190.00 3 Mean 314.88 308.42 329.94 330.27 4 Standard Deviasi 73.77 69.25 77.44 73.60 5 Variasi 5442.19 4795.64 5996.80 5416.97 6 Quantile 1 253.00 250.00 264.00 271.25 7 Quantile 2 324.00 309.00 338.00 340.50 8 Quantile 3 371.50 363.00 393.50 387.25 9 Skewnes -0.10 0.12 -0.14 -0.26 10 Median 324.00 309.00 338.00 340.50 11 Modus 199.00 257.00 346.00 326.00 Mencari Outlier \u00b6 Outlier adalah kesalahan dalam mengambil data. Sehingga terjadi penyimpangan dari data yang telah ditentukan syarat-syaratnya. Biasanya, digunakan formula Z untuk Standardisasi data. $$ z=\\frac{(X-\\mu)}{\\sigma} $$ Berhubung data yang saya lakukan tidak ada yang menyimpang, menjadikan data saya tidak akan bisa memunculkan Outlier. Source \u00b6 Untuk melihat data saya langsung, bisa klik Disini Referensi: \u00b6 smartstat.wordpress.com/2010/03/22/statistika-deskriptif/ konsultanstatistik.com/2009/03/uji-normalitas-dengan-skewness-dan.html rumusstatistik.com/2013/07/varian-dan-standar-deviasi-simpangan.html rumusbilangan.com/rumus-kuartil/ htpps://rumus.co.id/statistik-deskriptif/ rumusbilangan.com/rumus-kuartil/","title":"Statistik Deskriptif"},{"location":"Statistik/#statistika-deskriptif","text":"","title":"Statistika Deskriptif"},{"location":"Statistik/#pengertian-statistika","text":"Statistika yaitu merupakan suatu ilmu yang mempelajari tentang cara-cara pengumpulan data, penyajian data, dan juga analisis serta interpretasi tentang data tersebut sehingga menjadi suatu informasi yang berguna bagi pengambilan keputusan. Diperlukan sebuah metode. Metode Statistika adalah prosedur-prosedur yang digunakan dalam pengumpulan, penyajian, analisis, dan penafsiran data. \u200b","title":"Pengertian Statistika"},{"location":"Statistik/#pengertian-statistika-deskriptif","text":"Statistik yang menggambarkan kegiatan berupa pengumpulan data, penyusunan data, pengelolaan data, dan penyajian data dalam bentuk tabel, grafik, ataupun diagram, agar memberikan gambaran yang teratur ringkas, dan jelas mengenai suatu keadaan atau peristiwa. (M. Subana dkk, 2000;12) Deskriptif sendiri memiliki sifat menggambarkan atau mendeskripsikan suatu kondisi. Statistika deskriptif memiliki fungsi mempelajari tata cara pengumpulan, pencatatan, penyusunan, dan penyajian data penelitian dalam bentuk tabel frekuensi atau grafik, dan selanjutnya dilakukan pengukuran nilai-nilai statistiknya seperti mean/ rata-rata. \u200b","title":"Pengertian Statistika Deskriptif"},{"location":"Statistik/#distribusi-data","text":"Pengaturan, penyusunan, dan peringkasan data dengan membuat tabel seringkali membantu, terutama pada saat kita bekerja dengan sejumlah data yang besar. Tabel tersebut berisi daftar nilai yang mungkin berbeda (baik data tunggal maupun data berkelompok ) beserta nilai frekuensinya. Frekuensi adalah banyaknya kejadian/munculnya nilai data dengan kategori tertentu. Data yang telah diatur disebut **distribusi frekuensi**. Distribusi frekuensi dapat digambarkan dengan dua cara, yaitu sebagai **tabel** ataupun sebuah **grafik**. Grafik data kuantitatif dapat meliputi Histogram , Poligon frekuensi , dll. Sedangkan grafik untuk data kualitatif meliputi Bar Chart , Pie Chart , dll.","title":"Distribusi Data"},{"location":"Statistik/#ukuran-pemusatan","text":"","title":"Ukuran Pemusatan"},{"location":"Statistik/#mean","text":"Merupakan metode yang paling banyak digunakan untuk menggambarkan ukuran tendensi sentral. Mean dihitung dengan menjumlahkan semua nilai data lalu dibagi dengan banyaknya data tersebut. Berikut untuk rumus Mean: $$ \\overline{x}=\\frac{x_{1} f_{1}+x_{2} f_{2}+\\ldots+x_{n} f_{n}}{f_{1}+f_{2}+\\ldots+f_{n}} \\text { atau } \\quad \\overline{x}=\\frac{\\sum_{i=1}^{n} x_{i} f_{i}}{\\sum_{i=1}^{n} f_{i}} $$ keterangan: x bar = hasil mean xi .fi = jumlah data fi = banyak data","title":"Mean"},{"location":"Statistik/#mode","text":"Mode atau yang lebih sering dikenal dengan modus, adalah data yang sering muncul. Penentuannya dengan menyusun data dari kecil ke besar atau sebaliknya, kemudian hitung berapa banyak data yang sama tersebut. Berikut untuk rumus mode : $$ Modus =t b+\\left(\\frac{\\Delta F 1}{\\Delta F 1+\\Delta F 2}\\right) \\mathrm{p} $$ Keterangan: Modus = Modus tb = tepi bawah delta F1 = Frekuensi tertinggi - frekuensi diatasnya delta F2 = Frekuensi tertinggi - frekuensi diatasnya p = interval","title":"Mode"},{"location":"Statistik/#median","text":"Median adalah nilai tengah. DImana pengkuran pemusatan ini dengan membagi atau mencari nilai tengahnya, atau data yang berada di tengah. Berikut untuk rumus median: $$ M e=Q_{2}=\\left(\\frac{x_{n+1}}{2}\\right), jikanganjil $$ Keterangan: Q2 = Quartal 2 ( 50%) $$ M e=Q_{2}=\\left(\\frac{\\frac{x_{n} x_{n+1}}{2}}{2}\\right), jikangenap $$ Keterangan: perlu dibagi dua untuk menemukan nilai tengah","title":"Median"},{"location":"Statistik/#skewness","text":"Dibutuhkan karena memiliki manfaat tersendiri, yaitu akan mengetahui grafik normalitas melenceng ke kanan atau kekiri, mengumpul terlalu ditengah atau terlalu datar. Biasa juga disebut dengan ukuran kemencengan data. Rumus skewness: $$ \\mathrm{Sk}=\\frac{\\overline{\\mathrm{x}}-\\mathrm{Mo}}{\\mathrm{s}} $$ Keterangan: Sk = Skewness x = rata-rata Mo = Modus s = Simpangan baku","title":"Skewness"},{"location":"Statistik/#standar-deviasi","text":"Adalah nilai yang digunakan untuk menentukan sebaran data sampel, serta untuk mengetahui seberapa dekat titik titik antara mean (rata-rata) nilai sampel. Untuk menghitung standar deviasi, ahli statistik pertama-tama menghitung nilai rata-rata dari semua titik data. Deviasi yang telah dihasilkan dikenal dengan variasi. Deviasi adalah akar kuadrat dari varians. Rumus Standar Deviasi: $$ s=\\sqrt{s^{2}} $$ Keterangan: s = Standar Deviasi s^2 = varians s=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\overline{x}\\right)^{2}} s=\\sqrt{\\frac{1}{N-1} \\sum_{i=1}^{N}\\left(x_{i}-\\overline{x}\\right)^{2}} Keterangan: s = Standar Deviasi N = Jumlah data i = nomor data xi = data ke-i x bar = rata-rata sampel","title":"Standar Deviasi"},{"location":"Statistik/#varians","text":"Merupakan ukuran penyimpangan(selisih) antara nilai normal yang diharapkan menjadi kenyataan. Rumus Varians: s^{2}=\\frac{n \\sum_{i=1}^{n} x_{i}^{2}-\\left(\\sum_{i=1}^{n} x_{1}\\right)^{2}}{n(n-1)} s^{2}=\\frac{n \\sum_{i=1}^{n} x_{i}^{2}-\\left(\\sum_{i=1}^{n} x_{1}\\right)^{2}}{n(n-1)} Keterangan: s^2 : variasi n = banyaknya data xi^2 = data ke-i kuadrat","title":"Varians"},{"location":"Statistik/#quartile","text":"Kuartil adalah suatu rumus yang dapat memisahkan tiga bagian. Yang pertama ada kuartil bawah(Q1). Kuartil bawah diambil dari banyaknya data dikali 0.25. Kuartil tengah dikali 0.5 Kuartil atas dikali 0.75. Rumus mencari kuartil: Q_{i}=\\frac{i(n+1)}{4} Q_{i}=\\frac{i(n+1)}{4} Keterangan: untuk menentukan Quartil 1 i = 1 $$ \\quad Q_{i}=\\frac{i(n+1)}{4} $$ Keterangan: untuk menentukan Quartil 2 i = 2 $$ \\quad Q_{i}=\\frac{i(n+1)}{4}$ $$ Keterangan: untuk menentukan Quartil 3 i = 3","title":"Quartile"},{"location":"Statistik/#penerapan-statistika-deskriptif-dengan-python","text":"","title":"Penerapan Statistika Deskriptif dengan Python"},{"location":"Statistik/#alat-dan-bahan","text":"Text Editor (Jupyter Notebook paling direkomendasikan) Library Pandas (untuk statistika) Library scipy (untuk algoritma serta fungsi matematika)","title":"Alat dan Bahan"},{"location":"Statistik/#pertama","text":"import pandas as pd from scipy import stats","title":"Pertama"},{"location":"Statistik/#kedua","text":"Import file data yang akan dieksekusi (direkomendasikan menggunakan format .csv) df = pd . read_csv ( 'dataPWJT.csv' )","title":"Kedua"},{"location":"Statistik/#ketiga","text":"Membuat tampungan data dengan dictionary lalu melakukan iterasi untuk mendapatkan data-data setiap barisnya. data = { \"Statistics\" : [ 'Count' , 'Max' , 'Min' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Skewnes' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . count (), df [ i ] . max (), df [ i ] . min (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), round ( df [ i ] . skew (), 2 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]]","title":"Ketiga"},{"location":"Statistik/#keempat","text":"Eksekusi dengan dataframe agar data yang dihasilkan dapat dilihat dengan baik. eksekusi = pd . DataFrame ( data , columns = [ 'Statistics' ] + [ x for x in df . columns ]) eksekusi Hasilnya: Statistics Jatim Park 1 Jatim Park 2 Jatim Park 3 Kenjeran Park 0 Count 100.00 100.00 100.00 100.00 1 Max 448.00 450.00 450.00 449.00 2 Min 186.00 188.00 190.00 190.00 3 Mean 314.88 308.42 329.94 330.27 4 Standard Deviasi 73.77 69.25 77.44 73.60 5 Variasi 5442.19 4795.64 5996.80 5416.97 6 Quantile 1 253.00 250.00 264.00 271.25 7 Quantile 2 324.00 309.00 338.00 340.50 8 Quantile 3 371.50 363.00 393.50 387.25 9 Skewnes -0.10 0.12 -0.14 -0.26 10 Median 324.00 309.00 338.00 340.50 11 Modus 199.00 257.00 346.00 326.00","title":"Keempat"},{"location":"Statistik/#mencari-outlier","text":"Outlier adalah kesalahan dalam mengambil data. Sehingga terjadi penyimpangan dari data yang telah ditentukan syarat-syaratnya. Biasanya, digunakan formula Z untuk Standardisasi data. $$ z=\\frac{(X-\\mu)}{\\sigma} $$ Berhubung data yang saya lakukan tidak ada yang menyimpang, menjadikan data saya tidak akan bisa memunculkan Outlier.","title":"Mencari Outlier"},{"location":"Statistik/#source","text":"Untuk melihat data saya langsung, bisa klik Disini","title":"Source"},{"location":"Statistik/#referensi","text":"smartstat.wordpress.com/2010/03/22/statistika-deskriptif/ konsultanstatistik.com/2009/03/uji-normalitas-dengan-skewness-dan.html rumusstatistik.com/2013/07/varian-dan-standar-deviasi-simpangan.html rumusbilangan.com/rumus-kuartil/ htpps://rumus.co.id/statistik-deskriptif/ rumusbilangan.com/rumus-kuartil/","title":"Referensi:"},{"location":"clustering/","text":"CLUSTERING CATEGORICAL DATA \u00b6 Clustering adalah sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum. Metode K-Means Clustering \u00b6 K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi.Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Karakteristik dari algoritma ini adalah : Memiliki n buah data Input berupa jumlah data dan jumlah cluster (kelompok) Pada setiap cluster / kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u00b6 Secara sederhana algoritma K-Means dimulai dari tahap berikut : Tentukan titik yang akan di jadikan jumlah cluster. Alokasikan data ke dalam cluster secara random. Hitung centroid/rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/rata-rata terdekat. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan Permasalahan yang terdapat pada K-Means \u00b6 Ditemukannya beberapa model clustering yang berbeda Pemilihan jumlah cluster yang paling tepat Kegagalan untuk converge Outliers Bentuk cluster Overlapping Karakterristik K-Mein \u00b6 K-Means sangat cepat dalam proses clustering K-Means sangat sensitif pada pembangkitan centroid awal secara random Memungkinkan suatu cluster tidak mempunyai anggota Hasil clustering dengan K-Means bersifat tidak unik (selalu berubah-ubah) \u2013 terkadang baik, terkadang jelek K-means sangat sulit untuk mencapai global optimum Rumus K-Means \u00b6 Metode K-Modes \u00b6 K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Rumus K-Modes \u00b6 Metode K-Prototype \u00b6 Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Rumus K-Prototype \u00b6 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"clustering/#clustering-categorical-data","text":"Clustering adalah sebuah proses untuk mengelompokan data ke dalam beberapa cluster atau kelompok sehingga data dalam satu cluster memiliki tingkat kemiripan yang maksimum dan data antar cluster memiliki kemiripan yang minimum.","title":"CLUSTERING CATEGORICAL DATA"},{"location":"clustering/#metode-k-means-clustering","text":"K-Means Clustering adalah suatu metode penganalisaan data atau metode Data Mining yang melakukan proses pemodelan tanpa supervisi (unsupervised) dan merupakan salah satu metode yang melakukan pengelompokan data dengan sistem partisi.Metode K-Means Clustering berusaha mengelompokkan data yang ada ke dalam beberapa kelompok, dimana data dalam satu kelompok mempunyai karakteristik yang sama satu sama lainnya dan mempunyai karakteristik yang berbeda dengan data yang ada di dalam kelompok yang lain. Karakteristik dari algoritma ini adalah : Memiliki n buah data Input berupa jumlah data dan jumlah cluster (kelompok) Pada setiap cluster / kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut.","title":"Metode K-Means Clustering"},{"location":"clustering/#algoritma-k-means","text":"Secara sederhana algoritma K-Means dimulai dari tahap berikut : Tentukan titik yang akan di jadikan jumlah cluster. Alokasikan data ke dalam cluster secara random. Hitung centroid/rata-rata dari data yang ada di masing-masing cluster. Alokasikan masing-masing data ke centroid/rata-rata terdekat. Kembali ke Step 3, apabila masih ada data yang berpindah cluster atau apabila perubahan nilai centroid, ada yang di atas nilai threshold yang ditentukan atau apabila perubahan nilai pada objective function yang digunakan di atas nilai threshold yang ditentukan","title":"Algoritma K-Means"},{"location":"clustering/#permasalahan-yang-terdapat-pada-k-means","text":"Ditemukannya beberapa model clustering yang berbeda Pemilihan jumlah cluster yang paling tepat Kegagalan untuk converge Outliers Bentuk cluster Overlapping","title":"Permasalahan yang terdapat pada K-Means"},{"location":"clustering/#karakterristik-k-mein","text":"K-Means sangat cepat dalam proses clustering K-Means sangat sensitif pada pembangkitan centroid awal secara random Memungkinkan suatu cluster tidak mempunyai anggota Hasil clustering dengan K-Means bersifat tidak unik (selalu berubah-ubah) \u2013 terkadang baik, terkadang jelek K-means sangat sulit untuk mencapai global optimum","title":"Karakterristik  K-Mein"},{"location":"clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"clustering/#metode-k-modes","text":"K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"clustering/#rumus-k-modes","text":"","title":"Rumus K-Modes"},{"location":"clustering/#metode-k-prototype","text":"Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"clustering/#rumus-k-prototype","text":"MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Rumus K-Prototype"},{"location":"fuzzy clustering/","text":"fuzzy clustering \u00b6 pengertian fuzzy clustering: \u00b6 Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"Fuzzy C-Mean"},{"location":"fuzzy clustering/#fuzzy-clustering","text":"","title":"fuzzy clustering"},{"location":"fuzzy clustering/#pengertian-fuzzy-clustering","text":"Fuzzy C-Means (FCM) merupakan teknik meengelompokan data yang keberadaan data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu berikut adalah algoriyma dari FCM: https://docs.google.com/spreadsheets/d/1yD6loNq8VoutgNbvjuEeGCqHpgYzwabY/edit#gid=1490456583 berikut contoh code fuzzy C-Means from __future__ import division , print_function import numpy as np import matplotlib.pyplot as plt import skfuzzy as fuzz colors = [ 'b' , 'orange' , 'g' , 'r' , 'c' , 'm' , 'y' , 'k' , 'Brown' , 'ForestGreen' ] centers = [[ 4 , 2 ], [ 1 , 7 ], [ 5 , 6 ]] sigmas = [[ 0.8 , 0.3 ], [ 0.3 , 0.5 ], [ 1.1 , 0.7 ]] np . random . seed ( 42 ) xpts = np . zeros ( 1 ) ypts = np . zeros ( 1 ) labels = np . zeros ( 1 ) for i , (( xmu , ymu ), ( xsigma , ysigma )) in enumerate ( zip ( centers , sigmas )): xpts = np . hstack (( xpts , np . random . standard_normal ( 200 ) * xsigma + xmu )) ypts = np . hstack (( ypts , np . random . standard_normal ( 200 ) * ysigma + ymu )) labels = np . hstack (( labels , np . ones ( 200 ) * i )) fig0 , ax0 = plt . subplots () for label in range ( 3 ): ax0 . plot ( xpts [ labels == label ], ypts [ labels == label ], '.' , color = colors [ label ]) ax0 . set_title ( 'Test data: 200 points x3 clusters.' ) fig1 , axes1 = plt . subplots ( 3 , 3 , figsize = ( 8 , 8 )) alldata = np . vstack (( xpts , ypts )) fpcs = [] for ncenters , ax in enumerate ( axes1 . reshape ( - 1 ), 2 ): cntr , u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans ( alldata , ncenters , 2 , error = 0.005 , maxiter = 1000 , init = None ) fpcs . append ( fpc ) cluster_membership = np . argmax ( u , axis = 0 ) for j in range ( ncenters ): ax . plot ( xpts [ cluster_membership == j ], ypts [ cluster_membership == j ], '.' , color = colors [ j ]) # Mark the center of each fuzzy cluster for pt in cntr : ax . plot ( pt [ 0 ], pt [ 1 ], 'rs' ) ax . set_title ( 'Centers = {0}; FPC = {1:.2f}' . format ( ncenters , fpc )) ax . axis ( 'off' ) fig1 . tight_layout () fig2 , ax2 = plt . subplots () ax2 . plot ( np . r_ [ 2 : 11 ], fpcs ) ax2 . set_xlabel ( \"Number of centers\" ) ax2 . set_ylabel ( \"Fuzzy partition coefficient\" ) cntr , u_orig , _ , _ , _ , _ , _ = fuzz . cluster . cmeans ( alldata , 3 , 2 , error = 0.005 , maxiter = 1000 ) # Show 3-cluster model fig2 , ax2 = plt . subplots () ax2 . set_title ( 'Trained model' ) for j in range ( 3 ): ax2 . plot ( alldata [ 0 , u_orig . argmax ( axis = 0 ) == j ], alldata [ 1 , u_orig . argmax ( axis = 0 ) == j ], 'o' , label = 'series ' + str ( j )) ax2 . legend () newdata = np . random . uniform ( 0 , 1 , ( 1100 , 2 )) * 10 u , u0 , d , jm , p , fpc = fuzz . cluster . cmeans_predict ( newdata . T , cntr , 2 , error = 0.005 , maxiter = 1000 ) cluster_membership = np . argmax ( u , axis = 0 ) fig3 , ax3 = plt . subplots () ax3 . set_title ( 'Random points classifed according to known centers' ) for j in range ( 3 ): ax3 . plot ( newdata [ cluster_membership == j , 0 ], newdata [ cluster_membership == j , 1 ], 'o' , label = 'series ' + str ( j )) ax3 . legend () plt . show ()","title":"pengertian fuzzy clustering:"},{"location":"missing values/","text":"Apa sih Missing Values itu? Missing Values \u00b6 Missing Value adalah informasi yang tidak tersedia untuk sebuah objek (kasus). Missing value terjadi karena informasi untuk sesuatu tentang objek tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Metode-metode untuk mengatasi Missing Values \u00b6 * Prosedur Berbasis Unit yang lengkap(Completly Recorded Units) \u00b6 \u200b Pada prosedur ini, analisis hanya dilakukan terhadap data yang lengkap. Sedangkan yang tidak lengkap akan diabaikan, atau bahkan dihilangkan. \u200b Metode ini memuaskan bila jumlah missing tidak terlalu besar, tapi prosedur ini menjadi tidak efisien jika persentase missing data (n2/2).100 meningkat. * Prosedur Berbasis Imputasi \u00b6 Imputasi merupakan cara yang umum dan fleksibel. Biasanya pemegang data mengganti dengan nilai-nilai yang berdekatan, atau dengan memasukkan nilai rata-rata yang telah ditentukan sebelumnya Menentukan Missing Value dengan konsep K- Nearest Neighbor(KNN) \u00b6 Cara KNN sendiri adalah dengan menyamakan dengan nilai-nilai terdekatnya dari jenis nilai yang lain. Berikut tampilan dari data yang saya jadikan contoh: No. GameID LeagueIndex Age HoursPerWeek TotalHours APM SelectByHotkeys AssignToHotkeys UniqueHotkeys MinimapAttacks MinimapRightClicks NumberOfPACs GapBetweenPACs ActionLatency ActionsInPAC TotalMapExplored WorkersMade UniqueUnitsMade ComplexUnitsMade ComplexAbilitiesUsed 3365 10050 8 ? ? ? 211.9188 0.019817 0.000633 4 0.000201 0.000201 0.003912 31.8222 54.5588 5.0294 14 0.001409 3 0.000000 0.000000 3366 10051 8 ? ? ? 269.8998 0.024645 0.000642 10 0.000415 0.000491 0.004015 25.6352 43.3856 6.4922 21 0.000478 6 0.000000 0.000579 3367 10052 8 ? ? ? 190.2396 0.008720 0.000879 10 0.000171 0.000342 0.004971 17.9901 35.9509 5.5872 21 0.000904 5 0.000000 0.000000 3368 10055 8 ? ? ? 212.4972 0.014917 0.000767 10 0.000599 0.000273 0.005648 21.6687 41.2231 4.4680 28 0.001119 9 0.000035 0.000062 3369 10059 8 ? ? ? 219.3894 0.005926 0.000741 6 0.000440 0.000709 0.005185 17.0456 30.5342 6.6749 35 0.002072 9 0.000225 0.000064 3370 10060 8 ? ? ? 230.6694 0.010383 0.001242 10 0.000375 0.003328 0.006375 13.5028 31.4044 5.0533 32 0.001512 6 0.000035 0.000047 3371 10061 8 ? ? ? 284.2296 0.016069 0.000711 9 0.000355 0.000548 0.006680 9.4756 29.6851 5.3326 25 0.002459 7 0.000000 0.000000 3372 10062 8 ? ? ? 355.3518 0.037526 0.000600 7 0.001242 0.000514 0.004541 9.2871 41.9497 6.5063 22 0.001228 8 0.000000 0.000614 3373 10063 8 ? ? ? 364.8504 0.042576 0.000996 8 0.000176 0.000146 0.004687 19.9499 41.1417 5.6167 18 0.000674 7 0.000000 0.000000 3374 10064 8 ? ? ? 256.5888 0.019592 0.000580 8 0.000416 0.000357 0.005812 17.0462 34.3734 5.0563 19 0.001308 7 0.000000 0.000000 3375 10065 8 ? ? ? 248.4012 0.016018 0.000874 9 0.000388 0.000372 0.005987 16.3144 30.2486 5.0973 21 0.001197 6 0.000000 0.000000 3376 10066 8 ? ? ? 251.2284 0.022910 0.000946 5 0.001097 0.001173 0.005411 13.7404 35.7203 4.5524 22 0.000738 5 0.000000 0.000662 Terlihat bahwa ada tanda tanya (?) di dalam tabel tesebut. Itulah yang dinamakan Missing Value(Data Hilang) Sehingga kita perlu menggunakan K-Nearest Neighbor untuk menuntaskan masalah tersebut. Sumber data yang saya ambil: \u00b6 http://archive.ics.uci.edu/ml/datasets/skillcraft1+master+table+dataset","title":"Imputasi Data Hilang(Missing Values) Menggunakan KNN"},{"location":"missing values/#missing-values","text":"Missing Value adalah informasi yang tidak tersedia untuk sebuah objek (kasus). Missing value terjadi karena informasi untuk sesuatu tentang objek tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada.","title":"Missing Values"},{"location":"missing values/#metode-metode-untuk-mengatasi-missing-values","text":"","title":"Metode-metode untuk mengatasi Missing Values"},{"location":"missing values/#prosedur-berbasis-unit-yang-lengkapcompletly-recorded-units","text":"\u200b Pada prosedur ini, analisis hanya dilakukan terhadap data yang lengkap. Sedangkan yang tidak lengkap akan diabaikan, atau bahkan dihilangkan. \u200b Metode ini memuaskan bila jumlah missing tidak terlalu besar, tapi prosedur ini menjadi tidak efisien jika persentase missing data (n2/2).100 meningkat.","title":"* Prosedur Berbasis Unit yang lengkap(Completly Recorded Units)"},{"location":"missing values/#prosedur-berbasis-imputasi","text":"Imputasi merupakan cara yang umum dan fleksibel. Biasanya pemegang data mengganti dengan nilai-nilai yang berdekatan, atau dengan memasukkan nilai rata-rata yang telah ditentukan sebelumnya","title":"* Prosedur Berbasis Imputasi"},{"location":"missing values/#menentukan-missing-value-dengan-konsep-k-nearest-neighborknn","text":"Cara KNN sendiri adalah dengan menyamakan dengan nilai-nilai terdekatnya dari jenis nilai yang lain. Berikut tampilan dari data yang saya jadikan contoh: No. GameID LeagueIndex Age HoursPerWeek TotalHours APM SelectByHotkeys AssignToHotkeys UniqueHotkeys MinimapAttacks MinimapRightClicks NumberOfPACs GapBetweenPACs ActionLatency ActionsInPAC TotalMapExplored WorkersMade UniqueUnitsMade ComplexUnitsMade ComplexAbilitiesUsed 3365 10050 8 ? ? ? 211.9188 0.019817 0.000633 4 0.000201 0.000201 0.003912 31.8222 54.5588 5.0294 14 0.001409 3 0.000000 0.000000 3366 10051 8 ? ? ? 269.8998 0.024645 0.000642 10 0.000415 0.000491 0.004015 25.6352 43.3856 6.4922 21 0.000478 6 0.000000 0.000579 3367 10052 8 ? ? ? 190.2396 0.008720 0.000879 10 0.000171 0.000342 0.004971 17.9901 35.9509 5.5872 21 0.000904 5 0.000000 0.000000 3368 10055 8 ? ? ? 212.4972 0.014917 0.000767 10 0.000599 0.000273 0.005648 21.6687 41.2231 4.4680 28 0.001119 9 0.000035 0.000062 3369 10059 8 ? ? ? 219.3894 0.005926 0.000741 6 0.000440 0.000709 0.005185 17.0456 30.5342 6.6749 35 0.002072 9 0.000225 0.000064 3370 10060 8 ? ? ? 230.6694 0.010383 0.001242 10 0.000375 0.003328 0.006375 13.5028 31.4044 5.0533 32 0.001512 6 0.000035 0.000047 3371 10061 8 ? ? ? 284.2296 0.016069 0.000711 9 0.000355 0.000548 0.006680 9.4756 29.6851 5.3326 25 0.002459 7 0.000000 0.000000 3372 10062 8 ? ? ? 355.3518 0.037526 0.000600 7 0.001242 0.000514 0.004541 9.2871 41.9497 6.5063 22 0.001228 8 0.000000 0.000614 3373 10063 8 ? ? ? 364.8504 0.042576 0.000996 8 0.000176 0.000146 0.004687 19.9499 41.1417 5.6167 18 0.000674 7 0.000000 0.000000 3374 10064 8 ? ? ? 256.5888 0.019592 0.000580 8 0.000416 0.000357 0.005812 17.0462 34.3734 5.0563 19 0.001308 7 0.000000 0.000000 3375 10065 8 ? ? ? 248.4012 0.016018 0.000874 9 0.000388 0.000372 0.005987 16.3144 30.2486 5.0973 21 0.001197 6 0.000000 0.000000 3376 10066 8 ? ? ? 251.2284 0.022910 0.000946 5 0.001097 0.001173 0.005411 13.7404 35.7203 4.5524 22 0.000738 5 0.000000 0.000662 Terlihat bahwa ada tanda tanya (?) di dalam tabel tesebut. Itulah yang dinamakan Missing Value(Data Hilang) Sehingga kita perlu menggunakan K-Nearest Neighbor untuk menuntaskan masalah tersebut.","title":"Menentukan Missing Value dengan konsep K- Nearest Neighbor(KNN)"},{"location":"missing values/#sumber-data-yang-saya-ambil","text":"http://archive.ics.uci.edu/ml/datasets/skillcraft1+master+table+dataset","title":"Sumber data yang saya ambil:"},{"location":"pohon keputusan/","text":"DECISION TREE \u00b6 Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Entropy \u00b6 Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row Gain \u00b6 Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 ## GINI INDEX Dalam penerapan GINI index untuk data berskala continuous, terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode *brute-force* dan metode *midpoints*. # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata (): balance_data = pd . read_csv ( 'https://archive.ics.uci.edu/ml/machine-learning-' + 'databases/balance-scale/balance-scale.data' , sep = ',' , header = None ) # Printing the dataswet shape print ( \"Dataset Length: \" , len ( balance_data )) print ( \"Dataset Shape: \" , balance_data . shape ) # Printing the dataset obseravtions print ( \"Dataset: \" , balance_data . head ()) return balance_data # Function to split the dataset def splitdataset ( balance_data ): # Seperating the target variable X = balance_data . values [:, 1 : 5 ] Y = balance_data . values [:, 0 ] # Spliting the dataset into train and test X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) return X , Y , X_train , X_test , y_train , y_test # Function to perform training with giniIndex. def train_using_gini ( X_train , X_test , y_train ): # Creating the classifier object clf_gini = DecisionTreeClassifier ( criterion = \"gini\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_gini . fit ( X_train , y_train ) return clf_gini # Function to perform training with entropy. def tarin_using_entropy ( X_train , X_test , y_train ): # Decision tree with entropy clf_entropy = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_entropy . fit ( X_train , y_train ) return clf_entropy # Function to make predictions def prediction ( X_test , clf_object ): # Predicton on test with giniIndex y_pred = clf_object . predict ( X_test ) print ( \"Predicted values:\" ) print ( y_pred ) return y_pred # Function to calculate accuracy def cal_accuracy ( y_test , y_pred ): print ( \"Confusion Matrix: \" , confusion_matrix ( y_test , y_pred )) print ( \"Accuracy : \" , accuracy_score ( y_test , y_pred ) * 100 ) print ( \"Report : \" , classification_report ( y_test , y_pred )) # Driver code def main (): # Building Phase data = importdata () X , Y , X_train , X_test , y_train , y_test = splitdataset ( data ) clf_gini = train_using_gini ( X_train , X_test , y_train ) clf_entropy = tarin_using_entropy ( X_train , X_test , y_train ) # Operational Phase print ( \"Results Using Gini Index:\" ) # Prediction using gini y_pred_gini = prediction ( X_test , clf_gini ) cal_accuracy ( y_test , y_pred_gini ) print ( \"Results Using Entropy:\" ) # Prediction using entropy y_pred_entropy = prediction ( X_test , clf_entropy ) cal_accuracy ( y_test , y_pred_entropy ) # Calling main function if __name__ == \"__main__\" : main () Dataset Length : 625 Dataset Shape : ( 625 , 5 ) Dataset : 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index : Predicted values : [ 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 67 18 ] [ 0 19 71 ]] Accuracy : 73.40425531914893 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy : Predicted values : [ 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 63 22 ] [ 0 20 70 ]] Accuracy : 70.74468085106383 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); ]","title":"Desicion Tree"},{"location":"pohon keputusan/#decision-tree","text":"Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain.","title":"DECISION TREE"},{"location":"pohon keputusan/#entropy","text":"Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row","title":"Entropy"},{"location":"pohon keputusan/#gain","text":"Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur","title":"Gain"},{"location":"pohon keputusan/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 ## GINI INDEX Dalam penerapan GINI index untuk data berskala continuous, terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode *brute-force* dan metode *midpoints*. # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata (): balance_data = pd . read_csv ( 'https://archive.ics.uci.edu/ml/machine-learning-' + 'databases/balance-scale/balance-scale.data' , sep = ',' , header = None ) # Printing the dataswet shape print ( \"Dataset Length: \" , len ( balance_data )) print ( \"Dataset Shape: \" , balance_data . shape ) # Printing the dataset obseravtions print ( \"Dataset: \" , balance_data . head ()) return balance_data # Function to split the dataset def splitdataset ( balance_data ): # Seperating the target variable X = balance_data . values [:, 1 : 5 ] Y = balance_data . values [:, 0 ] # Spliting the dataset into train and test X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) return X , Y , X_train , X_test , y_train , y_test # Function to perform training with giniIndex. def train_using_gini ( X_train , X_test , y_train ): # Creating the classifier object clf_gini = DecisionTreeClassifier ( criterion = \"gini\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_gini . fit ( X_train , y_train ) return clf_gini # Function to perform training with entropy. def tarin_using_entropy ( X_train , X_test , y_train ): # Decision tree with entropy clf_entropy = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_entropy . fit ( X_train , y_train ) return clf_entropy # Function to make predictions def prediction ( X_test , clf_object ): # Predicton on test with giniIndex y_pred = clf_object . predict ( X_test ) print ( \"Predicted values:\" ) print ( y_pred ) return y_pred # Function to calculate accuracy def cal_accuracy ( y_test , y_pred ): print ( \"Confusion Matrix: \" , confusion_matrix ( y_test , y_pred )) print ( \"Accuracy : \" , accuracy_score ( y_test , y_pred ) * 100 ) print ( \"Report : \" , classification_report ( y_test , y_pred )) # Driver code def main (): # Building Phase data = importdata () X , Y , X_train , X_test , y_train , y_test = splitdataset ( data ) clf_gini = train_using_gini ( X_train , X_test , y_train ) clf_entropy = tarin_using_entropy ( X_train , X_test , y_train ) # Operational Phase print ( \"Results Using Gini Index:\" ) # Prediction using gini y_pred_gini = prediction ( X_test , clf_gini ) cal_accuracy ( y_test , y_pred_gini ) print ( \"Results Using Entropy:\" ) # Prediction using entropy y_pred_entropy = prediction ( X_test , clf_entropy ) cal_accuracy ( y_test , y_pred_entropy ) # Calling main function if __name__ == \"__main__\" : main () Dataset Length : 625 Dataset Shape : ( 625 , 5 ) Dataset : 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index : Predicted values : [ 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 67 18 ] [ 0 19 71 ]] Accuracy : 73.40425531914893 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy : Predicted values : [ 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 63 22 ] [ 0 20 70 ]] Accuracy : 70.74468085106383 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); ]","title":"CARA MEMBUAT DECISION TREE"},{"location":"siti/","text":"Hai \u00b6 assap mamenn","title":"Siti"},{"location":"siti/#hai","text":"assap mamenn","title":"Hai"}]}